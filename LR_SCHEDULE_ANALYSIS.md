# Learning Rate Schedule Analysis for Graph_Free with GRU

## Problem: Why Was LR Constant?

Looking at the code history:

```python
# Line 936 in architecture_trainer.py (2025-12-17):
# "UPDATED 2025-12-17: Removed warmup for graph_free (use constant LR for simplicity)"
if self.architecture_config.name in ["attention", "mamba"]:  # graph_free excluded!
```

But there's a **contradiction** at line 1814:

```python
# Line 1814:
# "For graph_free: Warmup is CRITICAL to prevent KL explosion from aggressive exploration"
```

The warmup was removed for "simplicity" but this contributed to the policy collapse!

---

## Why Graph_Free SHOULD Use LR Schedule

### 1. Fresh Policy/Value Networks
```
BC Pretraining Transfer:
  âœ“ Feature extractor: 737,312 params (from BC)
  âœ— Policy network: Random initialization
  âœ— Value network: Random initialization
  âœ— GRU weights: Random initialization
```

**With fresh networks, early gradients are noisy and large!**

### 2. GRU Amplifies Gradient Noise

GRU has recurrent connections that compound gradients:
```
Standard MLP:   gradient âˆ loss
LSTM/GRU:       gradient âˆ loss Ã— sequence_length Ã— hidden_state_updates

For 512-step sequences:
  Early training: gradients can be 10-100x larger than steady-state
  This causes policy collapse if LR is too high at start
```

### 3. Evidence from Failed Training

Your failed run showed:
- **KL divergence exceeded 0.0334** (threshold 0.02)
- Started with constant **LR=3e-4** from step 0
- Policy collapsed within 4M steps

**The constant high LR caused the policy to update too aggressively before value network stabilized.**

### 4. Standard Practice in Recurrent RL

Papers on LSTM/GRU PPO typically use:
- Warmup: 10-30% of training
- Start at 0.01-0.1x base LR
- Ramp to full LR over warmup period
- Optional decay for fine-tuning

Examples:
- Attention is All You Need (Vaswani et al.): Warmup for 4000 steps
- PPO-LSTM (OpenAI): Warmup for first 10-20% of training

---

## Warmup Schedule Details

### Phase 1: Ultra-Slow Start (0-2.5% of training)
```
LR: 0.01x base â†’ 0.1x base
    1e-6 â†’ 1e-5 (for base_lr=1e-4)

Purpose: Allow value network to build initial estimates
Steps:   0 - 500K (for 20M total)
```

**Why ultra-slow?**
- Value network has NO information initially
- Advantages are pure noise (random value estimates)
- Need time to bootstrap value function

### Phase 2: Warmup Ramp (2.5-25% of training)
```
LR: 0.1x base â†’ 1.0x base
    1e-5 â†’ 1e-4 (for base_lr=1e-4)

Purpose: Gradually increase as value estimates stabilize
Steps:   500K - 5M (for 20M total)
```

**Why gradual?**
- Value network improving but not stable
- Policy can start learning more aggressively
- Prevents premature convergence during learning

### Phase 3: Linear Decay (25-100% of training)
```
LR: 1.0x base â†’ 0.01x base
    1e-4 â†’ 1e-5 (for base_lr=1e-4)

Purpose: Fine-tuning and convergence
Steps:   5M - 20M (for 20M total)
```

**Why decay?**
- Policy approaching optimal behavior
- Fine-tuning requires small updates
- Prevents oscillation around optimum

---

## Comparison: Constant vs Warmup

### Constant LR=3e-4 (Your Failed Run)
```
Steps 0-4M:     3e-4 (too aggressive at start)
Result:         Policy collapse at ~4M steps
KL:             Exceeded 0.0334 (67% over threshold)
Learning:       No improvement, entropy increased
```

### Constant LR=1e-4 (Conservative, No Schedule)
```
Steps 0-20M:    1e-4 (safe but suboptimal)
Pros:           Stable, won't collapse
Cons:           Slow initial learning, may need >20M steps
                No fine-tuning at end
```

### Warmup Schedule with base=1e-4 (Recommended)
```
Steps 0-500K:   1e-6 to 1e-5 (ultra-gentle start)
Steps 500K-5M:  1e-5 to 1e-4 (ramp up)
Steps 5M-20M:   1e-4 to 1e-5 (fine-tune)

Pros:           Safe start, efficient learning, automatic fine-tuning
Expected:       Stable learning with gradual convergence
```

---

## Expected Training Dynamics

### Phase 1: Ultra-Slow Start (0-500K steps)
```
Learning Rate:   1e-6 to 1e-5
KL Divergence:   < 0.01 (very small updates)
Behavior:        Policy barely changes, exploring randomly
Value Learning:  Bootstrapping value estimates
Goal:            Build stable value function foundation
```

### Phase 2: Warmup Ramp (500K-5M steps)
```
Learning Rate:   1e-5 to 1e-4 (increasing)
KL Divergence:   0.01 to 0.03 (growing but controlled)
Behavior:        Policy starts learning from PBRS gradient
Value Learning:  Refining estimates, variance decreasing
Goal:            Learn navigation basics, first successes
Expected:        Success rate reaches 5-15% by 5M steps
```

### Phase 3: Full Learning (5M-15M steps)
```
Learning Rate:   1e-4 (full strength)
KL Divergence:   0.02 to 0.04 (active learning)
Behavior:        Policy improving rapidly
Value Learning:  Accurate estimates, low variance
Goal:            Master level mechanics
Expected:        Success rate reaches 30-60%
```

### Phase 4: Fine-Tuning (15M-20M steps)
```
Learning Rate:   1e-4 to 1e-5 (decaying)
KL Divergence:   < 0.02 (small refinements)
Behavior:        Policy near-optimal, fine-tuning
Value Learning:  Converged
Goal:            Polish performance, maximize success rate
Expected:        Success rate 60-80%+
```

---

## Comparison with Other Architectures

### Why Attention/Mamba Already Had Warmup

```python
# Line 939 (OLD code):
if self.architecture_config.name in ["attention", "mamba"]:
    # These were already getting warmup
```

**Reasoning**:
- Attention: Transformers notoriously unstable without warmup (original paper used it)
- Mamba: SSM has selective memory, complex gradient flow
- **Graph_free was excluded** but shouldn't have been!

### GRU Shares Similar Issues

| Issue | LSTM | GRU | Attention | Mamba |
|-------|------|-----|-----------|-------|
| Recurrent gradients | âœ“ | âœ“ | âœ— | âœ“ |
| Sequence dependencies | âœ“ | âœ“ | âœ“ | âœ“ |
| Hidden state updates | âœ“ | âœ“ | âœ— | âœ“ |
| Gradient amplification | âœ“ | âœ“ | âœ“ | âœ“ |
| **Needs warmup** | **âœ“** | **âœ“** | **âœ“** | **âœ“** |

**GRU has 3/4 issues that benefit from warmup!**

---

## Recommendation: YES, Use Warmup Schedule

**For graph_free with GRU, LR schedule is HIGHLY RECOMMENDED:**

### Benefits:
1. âœ… Prevents early policy collapse (your main issue!)
2. âœ… Allows value network to stabilize first
3. âœ… Gradual ramp-up as networks improve
4. âœ… Automatic fine-tuning at end of training
5. âœ… Standard practice for recurrent architectures

### Minimal Downsides:
- Slightly slower initial learning (by design - safety first)
- 2.5% of training at ultra-low LR (necessary for stability)

### Evidence It Helps:
- Attention/Mamba use it successfully
- Your constant-LR run collapsed
- Standard practice in papers

---

## Fix Applied âœ…

**Changed**:
```python
# OLD (line 939):
if self.architecture_config.name in ["attention", "mamba"]:

# NEW (line 941):
if self.architecture_config.name in ["attention", "mamba", "graph_free"]:
```

**Result**: Graph_free will now use the same warmup schedule as attention/mamba.

---

## Summary

**Question**: Should graph_free with GRU use a static LR?

**Answer**: **NO!** It should use a warmup schedule because:

1. âœ… Policy/value start randomly initialized (BC only pretrains features)
2. âœ… GRU has recurrent connections that amplify early gradient noise
3. âœ… Your failed run with constant LR=3e-4 collapsed
4. âœ… Standard practice for recurrent architectures
5. âœ… Other recurrent architectures (attention, mamba) already use it

**Change Applied**: Re-enabled LR warmup for graph_free (was disabled 2025-12-17)

**Training will now**:
- Start ultra-gentle (1e-6) to prevent collapse
- Ramp up over 5M steps to 1e-4 as networks stabilize
- Decay to 1e-5 for fine-tuning

This should significantly improve training stability! ðŸš€
